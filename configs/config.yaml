# --- Base Settings ---
batch_size: 128  # 256 best
epochs: 1
learning_rate: 0.001 # 0.0001 best for static lr

datasets:
    CICIDS2017:
        num_class: 8
        classes:
            - 'Benign'
            - 'DoS GoldenEye'
            - 'DoS Hulk'
            - 'DoS Slowhttptest'
            - 'DoS slowloris'
            - 'FTP-Patator'
            - 'Heartbleed'
            - 'SSH-Patator'
        Task_A:
            num_class: 3
            classes:
                - 'Benign'
                - 'FTP-Patator'
                - 'SSH-Patator'
        Task_B:
            num_class: 6
            classes:
                - 'Benign'
                - 'DoS GoldenEye'
                - 'DoS Hulk'
                - 'DoS Slowhttptest'
                - 'DoS slowloris'
                - 'Heartbleed'
        Task_C:
            num_class: 4
            classes:
                - 'Benign'
                - 'Web Attack Brute Force'
                - 'Web Attack Sql Injection'
                - 'Web Attack XSS'
            
    UNSWNB15:
        num_class: 10
        classes:
            - 'Benign'
            - 'Analysis'
            - 'Backdoor'
            - 'DoS'
            - 'Exploits'
            - 'Fuzzers'
            - 'Generic'
            - 'Reconnaissance'
            - 'Shellcode'
            - 'Worms'
          
    NBIOT:
        num_class: 11
        classes:
            [
            "bashcombo",
            "bashjunk",
            "bashscan",
            "bashtcp",
            "bashudp",
            "benign",
            "miraiack",
            "miraiscan",
            "miraisyn",
            "miraiudp",
            "miraiudpplain"
            ]

# --- Model Configuration ---
model:
    dim: 32              # Embedding dimension
    depth: 6             # Number of transformer layers
    heads: 10            # Number of attention heads
    attn_dropout: 0.1    # Attention dropout rate
    ff_dropout: 0.1      # Feed-forward dropout rate
    dim_out: 2           # Initial output classes (auto-expanded)
